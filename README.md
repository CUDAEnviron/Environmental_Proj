# Deliverable 2
For this segment of the final project, I(Ashley) took on the square role, and continued the work on the machine learning model.  I found a new temperature dataset than the one used in Deliverable 1.  The old dataset only went until 2103, and this new one has entries until 2016.  This dataset was cleaned and then uploaded to the database into the ‘temperature_data’ table,
## Preliminary Data Preprocessing
After importing both tables from the database and converting them to dataframes in the ML_model notebook, each dataframe was cleaned before being merged for initial analysis. In the Co2 data, the NAs were dropped, as well as any entries in the ‘country’ column that didn’t represent an actual country.  A new clean Co2 dataframe was created with only the most relevant columns.  The temperature data was cleaned as well, and then merged with the clean co2 data.  This combined dataset was then examined to establish if there is a correlation between co2 emissions and a country’s gdp output.  To determine a baseline for whether a country is a disproportionate polluter, we broke out the data for 2016, the most recent year in the dataset. We exported it as a csv to get a full look at the data and pick a baseline for "bad" ratio of gdp to co2 per capita.  After looking at the 2016 data in Excel, a new column is created to indicate co2 production per unit of gdp, by dividing 'co2 per capita' by 'gdp per capita' (multiplied by 100000 to make the result more readable). A baseline of 300 is chosen, as a cursory analysis shows that numbers between the max and 300 encapsulate countries with economies known to pollute, as well as some countries that are a surprise, so it seems like a good place to begin our analysis. So, countries with a ratio of co2 to gdp over 300, will be labeled as 'high' and those below 300 will be labeled as 'low’ in the  ‘emission_ratio’ column.
To ready the data for a machine learning model, that categorical data was encoded. I used the Label Encoder method from the scikit learn library to encode any features where the data was a string, the 'country' and 'emission_ratio' columns. The data was changed from a string to a number for each unique entry.
## Feature Selection
The X variable, or independent variable, holds all the features that effect the y variable, or dependent variable, 'emission_ratio.' The X features include 'Country', 'Year', 'CO2', 'Population', 'GDP', 'Temperature', 'GDP_per_Capita', 'CO2_per_Capita', and 'CO2_per_unit_GDP'. The Emission_Ratio indicates the relationship for each country between GDP output and CO2 emissions per person. The Emission_Ratio is effected for each country by each feature held in the X variable.
## Splitting into Training/Testing
The data was split into training and testing sets using the train_test_split method from the scikit learn library. It was trained using the Balanced Random Forest Classifier from the imbalanced-learn package.
## Model Choice
We chose a balanced random forest model classifier for this analysis. A BRM was chosen because random forest models are efficient, are not prone to overfitting, and are able to rank feature importance. However, a BRM is something of a black box algorithm, so there is little control over what the model does.
A BRM is an ensemble classifier that works by combining multiple decision tree algorithms to improve accuracy. A random forest model randomly selects a subset of features from the training set and fits a decision tree to each one. By choosing different features for each tree, each tree is independent and improves the ensemble prediction. The Balanced RF works by performing a random undersampling of the majority class to correct any imbalance between classes. In this case, there was not an extreme imbalance between classes to begin with, but every little bit helps to improve accuracy.  
## Model Validation
Using the test data to validate the model shows that this model performs with 99% accuracy. The classification report, furthermore, shows that there is high accuracy for both the high and low classes, for a well-balanced model.

# Deliverable 1
For this first segment of the final project, I took on the triangle role, which deals with the provisional machine learning model.  To begin with, I took our dataset on co2 and combined it with another that had temperature data, to create a fuller picture.  I then did some work to make the data work better for supervised machine learning, by creating a column with a binary outcome, in this case ‘high’ and ‘low’.  This referenced whether a given country is determined to be have a high ratio of co2 emission to gdp (polluter) or a low ratio (less of a polluter).  

## Which model
For the initial, early model, I chose a logistic regression model.  I chose this model because our target variable is a binary outcome, which is ideal for logistic regression, and because we hope to make predictions on future emissions outcomes.  I hoped a logistic regression model be able to determine the relationship between our binary outcome of high/low pollution ratio and the factors contributing to it, which in this dataset are temperature, co2 emission, population, and various gdp calculations.

## Training the model
To train the model, I broke our variables into `X` and `y`, with `X` being all the features except for ‘emission_ratio’ and `y` being the ‘emission_ratio’ column with values of ‘high’ or ‘low.’  After encoding the categorical data, the model was trained using the train_test_split method from the scikit learn library.
## Model Accuracy
After running the training data through the logistic regression model, I made predictions on the test data `(X_test`) and compared it to the actual data `(y_test)` in a dataframe.  I also used the accuracy_score method from the scikit library, which showed a result of 0.9945, or 99%.
## How does this model work
This model works by analyzing the available data and then, when given new data, it determines the probability of the data belonging to one of two classes.  In this case, it used the training data to determine the factors contributing to a country being a ‘high’ or ‘low’ ratio polluter, and then when given the testing data, it was able to predict ‘high’ or ‘low’ with 99% accuracy.

